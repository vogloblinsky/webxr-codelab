<!doctype html>

<html>

<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Développez une application de réalité augmentée 100% Web</title>
  <script src="./bower_components/webcomponentsjs/webcomponents-lite.js"></script>
  <link rel="import" href="./elements/codelab.html">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <style is="custom-style">
    body {
      font-family: "Roboto",sans-serif;
      background: var(--google-codelab-background, #F8F9FA);
    }
  </style>

</head>

<body unresolved class="fullbleed">

  <google-codelab title="Développez une application de réalité augmentée 100% Web" environment="web" feedback-link="https://github.com/vogloblinsky/webxr-codelab/issues">

    <google-codelab-step label="Introduction" duration="5">
      <p>Ce codelab est un exemple pas à pas de conception d&#39;une application web AR. Elle utilise JavaScript pour
        réaliser le rendu de modèles 3D qui apparaissent comme s&#39;ils existaient dans la réalité.</p>
      <p>Vous allez utiliser l&#39;API en cours de développement <a href="https://immersive-web.github.io/webxr/"
          target="_blank">WebXR</a>, (l&#39;API qui succède à l&#39;API <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebVR_API"
          target="_blank">WebVR</a>), qui combine les fonctionnalitées de la réalité augmentée (AR) et la réalité
        virtuelle (VR). Nous allons nous focaliser sur les extensions expérimentales AR de l&#39;API WebXR Device qui
        sont en développement dans Chrome.</p>
      <p>Quelques slides sont disponibles sur ce lien pour une explication de cette API : <a href="https://slides.com/vogloblinsky/webxr-api-virtual-augmented-mixed-reality-and-the-immersive-web/"
          target="_blank">Slides</a></p>
      <h2><strong>Qu&#39;est-ce que la réalité augmentée?</strong></h2>
      <p>La réalitée augmentée (AR) est un terme généralement utilisé pour décrire le mélange de graphiques générés par
        ordinateur avec le monde réel, ce qui, dans le cas de la réalité augmentée avec un téléphone, veut dire placer
        de manière convaincante un graphique générée par ordinateur au-dessus du flux vidéo de l&#39;appareil photo.
        Pour que cet effet reste convaincant même quand le téléphone bouge, le téléphone doit connaître
        l&#39;environnement dans lequel il évolue, c&#39;est-à-dire détecter les surfaces et estimer l&#39;éclairage
        ambiant. En complément, le téléphone doit également déterminer sa &#34;position&#34; et son
        &#34;orientation&#34; dans ces environnement réel en 3 dimensions.</p>
      <p>L&#39;usage de la réalité augmentée est en constante augmentation, et avec un usage grandissant dans des
        applications populaires comme les filtres de &#34;selfies&#34; ou de jeux AR. Aujourd&#39;hui, le parc comprend
        des centaines de millions de téléphone compatibles avec la réalité augmentée, seulement un an après la sortie
        d&#39;<a href="https://developers.google.com/ar/discover/" target="_blank">ARCore</a>, la plateforme de réalité
        augmentée de Google, et <a href="https://developer.apple.com/arkit/" target="_blank">ARKit</a> d&#39;Apple.
        Avec cette technologie maintenant dans les mains de millions de personne, les propositions d&#39;extensions AR
        de l&#39;API WebXR peuvent être implémentées derrière des drapeaux dans les navigateurs.</p>
      <h2><strong>Ce que vous allez concevoir</strong></h2>
      <table>
        <tr>
          <td colspan="1" rowspan="1">
            <p>Dans ce codelab, nous allons concevoir une application qui va vous permettre de prévisualiser des plats
              dans un restaurant, en plaçant au-dessus de votre assiette un modèle en utilisant la réalité augmentée.
              Votre application va :</p>
            <ol type="1" start="1">
              <li>Utiliser les capteurs de votre téléphone pour déterminer et suivre sa position et son orientation
                dans le monde réel.</li>
              <li>Faire le rendu du modèle 3D au-dessus du flux vidéo de votre appareil photo.</li>
              <li>Exécuter des tests de collision pour placer des objets au-dessus de surfaces découvertes dans le
                monde réel.</li>
            </ol>
          </td>
          <td colspan="1" rowspan="1">
            <p><img style="max-width: 298.00px" src="img/5f9baa92feded1b2.png"></p>
          </td>
        </tr>
      </table>
      <h2><strong>Ce que vous allez apprendre</strong></h2>
      <ul>
        <li>Comment utiliser l&#39;API WebXR</li>
        <li>Comment trouver une surface en utilisant les tests de collisions de la réalité augmentée</li>
        <li>Comment charger et afficher un modèle 3D synchronisé avec le flux vidéo de l&#39;appareil photo</li>
      </ul>
      <p>Ce codelab est focalisé sur les APIs de réalité augmentée. Les concepts externes et non pertinents ne seront
        pas expliqués ici et fournis tel quel dans le dépôt de code.</p>


    </google-codelab-step>

    <google-codelab-step label="Mise en place" duration="15">
      <h2>⚠ Ne fonctionne pas dans les dernières versions de Chrome Dev et Canary ⚠</h2>
      <p>L&#39;API WebXR est en cours de développement et soumise régulièrement à des changements. Ce codelab a été
        testé avec la version 70-72 de Chrome Dev.</p>
      <h2><strong>Ce dont vous avez besoin</strong></h2>
      <p>Ceci est un aperçu de tout ce dont vous avez besoin, et nous verrons tout ceci en détail après :</p>
      <ul>
        <li>un PC de développement et un serveur local d&#39;hébergement type Apache, nginx ou http-server (au travers
          de Node.js/npm)</li>
        <li>un <a href="https://developers.google.com/ar/discover/supported-devices" target="_blank">téléphone
            compatible ARCore</a> tournant sous <a href="https://www.android.com/versions/oreo-8-0/" target="_blank">Android
            8.0 Oreo</a></li>
        <li><a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank">ARcore</a>
          installé sur le téléphone (Chrome vous proposera de l&#39;installer)</li>
        <li>Chrome Dev. Utilisez une version entre 70-72, et utilisez la version &#34;<strong>Dev build</strong>&#34;
          (les branchements ARCore ne sont pas encore livrés sur les versions beta/prod)</li>
        <li><a href="https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb"
            target="_blank">Web Server pour Chrome</a>, ou votre propre serveur web</li>
        <li>un câble USB reliant votre téléphone à votre PC</li>
        <li>l&#39;<a href="https://github.com/vogloblinsky/webxr-codelab" target="_blank">exemple de code</a> du
          codelab</li>
        <li>un éditeur de code</li>
        <li>des connaissances en HTML, CSS, JavaScript et <a href="https://developer.chrome.com/devtools" target="_blank">Chrome
            Devtools</a></li>
      </ul>
      <h2><strong>Obtenez Chrome et ses fonctionnalités AR</strong></h2>
      <p>A l&#39;heure de l&#39;écriture de ce codelab, les fonctionnalités AR sont implémentées dans Chrome Dev à
        partir de la version 70.</p>
      <p>Vous pouvez y accéder dans <strong>Paramètres</strong> -&gt; <strong>A propos de Chrome</strong> et voir la
        version de Chrome que vous utilisez.</p>
      <p>La version actuelle (74) de ChromeDev étant trop récente, veuillez installer Chrome Dev avec le lien
        ci-dessous :</p>
      <ul>
        <li>Chrome Dev : <a href="https://www.apkmirror.com/apk/google-inc/chrome-dev/chrome-dev-72-0-3626-14-release/"
            target="_blank">Chrome Dev 72.0.3626.14</a></li>
      </ul>
      <p>L&#39;apk est généralement en architecture armeabi-v7a.</p>
      <h2><strong>S&#39;assurer que les fonctionnalités AR sont activées dans Chrome</strong></h2>
      <p>A l&#39;heure de l&#39;écriture de ce codelab, les fonctionnalités AR sont implémentées derrière les drapeaux
        webxr et webxr-hit-test. Pour activer le support de l&#39;API WebXR dans Chrome :</p>
      <ol type="1" start="1">
        <li>vérifiez que votre téléphone fonctionne bien sous <a href="https://www.android.com/versions/oreo-8-0/"
            target="_blank">Android 8.0 Oreo</a></li>
        <li>vérifiez que votre téléphone est bien compatible ARCore <a href="https://developers.google.com/ar/discover/supported-devices"
            target="_blank">ici</a></li>
        <li>vérifiez que votre version de Chrome est entre 70 et 72</li>
        <li>tapez <code>chrome://flags</code> dans la barre d&#39;adresse</li>
        <li>tapez <code>webxr</code> dans le champ de recherche des drapeaux</li>
        <li>activez le drapeau WebXR Device API (<code>#webxr</code>)</li>
      </ol>
      <ul>
        <li>note : ignorez le drapeau WebVR (<code>#enable-webvr</code>)</li>
      </ul>
      <ol type="1" start="7">
        <li>activez le drapeau WebXR Hit Test (<code>#webxr-hit-test</code>)</li>
        <li>redémarrez Chrome pour vous assurez que les drapeaux sont actifs</li>
      </ol>
      <p><img style="max-width: 369.74px" src="img/59d4f183c4f0156.png"></p>
      <p>Visitez le lien ci-dessous sur votre téléphone pour essayer l&#39;étape 1 du codelab. Si vous obtenez une page
        avec un message &#34;Votre navigateur ne comporte pas les fonctionnalités AR&#34;, re-vérifiez la version de
        Chrome Canary et les drapeaux WebXR, qui requiert un redémarrage du navigateur.</p>
      <p><a href="https://vogloblinsky.github.io/webxr-codelab/work/" target="_blank">
          <paper-button class="colored" raised>Essayez</paper-button>
        </a></p>
      <h2><strong>Téléchargez le code source du codelab</strong></h2>
      <p>Cliquez sur le lien ci-dessous pour téléchargez le code source du codelab sur votre PC :</p>
      <p><a href="https://github.com/vogloblinsky/webxr-codelab/archive/master.zip" target="_blank">
          <paper-button class="colored" raised>Téléchargez le code source</paper-button>
        </a></p>
      <p>Dé-zippez ce fichier. Vous allez obtenir un dossier (<code>webxr-codelab</code>), qui contient dans des
        dossiers les différentes étapes du codelab, avec des ressources communes associées.</p>
      <p>Les dossiers <code>step-NN</code> contiennent les sources pour les étapes correspondantes du codelab. Ils
        sont là pour référence.</p>
      <p>Nous allons démarrer le codelab avec le dossier <code>work</code>.</p>
      <h2><strong>Installez et vérifiez votre serveur web</strong></h2>
      <p>Vous êtes libre d&#39;utiliser votre propre serveur web, mais nous allons voir ici comment utiliser Chrome Web
        Server si vous n&#39;en avez pas un. Si vous n&#39;avez pas cette application installée sur votre ordinateur,
        vous pouvez l&#39;installer depuis le Chrome Web Store.</p>
      <p><a href="https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb?hl=en"
          target="_blank">
          <paper-button class="colored" raised>Installez Web Server for Chrome</paper-button>
        </a></p>
      <p>Après l&#39;installation, cliquez sur le raccourci &#34;Applications&#34; dans un nouvel onglet Chrome.</p>
      <p><img style="max-width: 114.00px" src="img/dcd4593b8fc0aedf.png"></p>
      <p>Vous devriez voir apparaître cette fenêtre qui va vous permettre de configurer votre serveur local.</p>
      <p><img alt="Screen Shot 2016-02-18 at 11.48.14 AM.png" style="max-width: 513.64px" src="img/433870360ad308d4.png"></p>
      <ol type="1" start="1">
        <li>cliquez sur le bouton <strong>choose folder</strong>, et sélectionnez le dossier <code>webxr-codelab</code>.
          Cela va vous permettre d&#39;héberger votre travail en cours de développement via une addresse mise en avant
          dans la fenêtre (dans la section <strong>Web Server URL(s)</strong>).</li>
        <li>dans les options, vérifiez que <strong>Automatically show index.html</strong> est bien activée</li>
        <li><strong>ARRÊTEZ</strong> et <strong>REDÉMARREZ</strong> le serveur en activant le bouton glissant</li>
      </ol>
      <p><img alt="Screen Shot 2016-02-18 at 12.22.18 PM.png" style="max-width: 194.50px" src="img/daefd30e8a290df5.png"></p>
      <ol type="1" start="4">
        <li>Vérifiez qu&#39;au moins une adresse apparaît : </li>
      </ol>
      <ul>
        <li>http://127.0.0.1:8887 — l&#39;adresse par défaut localhost</li>
      </ul>
      <p>Maintenant nous allons configurer votre téléphone afin qu&#39;en visitant <code>localhost:8887</code> vous
        ayez accès au même port sur votre PC.</p>
      <ol type="1" start="1">
        <li>sur votre PC, dans Chrome, saisissez dans votre barre d&#39;adresse <code>chrome://inspect</code> et
          cliquez sur le bouton <strong>Port forwarding...</strong></li>
      </ol>
      <p><img style="max-width: 462.00px" src="img/edbc1c8e20fe77a1.png"></p>
      <p>Utilisez le paramètre <strong>Port forwarding settings</strong> pour transmettre le port <code>8887</code>
        vers <code>localhost:8887</code>. Vérifiez ensuite que l&#39;option <strong>Enable port forwarding</strong> est
        active.</p>
      <p><img style="max-width: 228.24px" src="img/8ceaaff488b3161.png"></p>
      <p>Activez le mode développeur sous Android :</p>
      <ol type="1" start="1">
        <li>rendez-vous dans les paramètres</li>
        <li>cliquez sur &#34;<strong>A propos du téléphone</strong>&#34;</li>
        <li>cliquez sur &#34;<strong>Informations sur le logiciel</strong>&#34;</li>
        <li>cliquez plusieurs fois sur le &#34;<strong>Numéro de build</strong>&#34; afin d&#39;activer le mode
          développeur</li>
      </ol>
      <p>Activez le débogage USB :</p>
      <ol type="1" start="1">
        <li>rendez-vous dans les paramètres</li>
        <li>cliquez sur &#34;<strong>Options de développement</strong>&#34;</li>
        <li>activez l&#39;option &#34;<strong>Débogage USB</strong>&#34;</li>
      </ol>
      <p>Testez votre connection :</p>
      <ol type="1" start="1">
        <li>connectez votre téléphone à votre PC avec votre câble USB</li>
        <li>sur votre téléphone, saisissez <code>http://localhost:8887</code> dans la barre d&#39;adresse.</li>
        <li>sur votre téléphone, cliquez sur le dossier <code>work</code> pour chargez la page <code>work/index.html</code></li>
      </ol>
      <table>
        <tr>
          <td colspan="1" rowspan="1">
            <p>Vous devez normalement voir cette page...</p>
          </td>
          <td colspan="1" rowspan="1">
            <p>...sinon, vérifiez la version de Chrome Canary, les drapeaux dans <code>chrome://flags</code> et
              redémarrez Chrome</p>
          </td>
        </tr>
        <tr>
          <td colspan="1" rowspan="1">
            <p><img style="max-width: 298.00px" src="img/d1ae982c33aa9663.png"></p>
          </td>
          <td colspan="1" rowspan="1">
            <p><img style="max-width: 298.00px" src="img/5636cebcb56c699.png"></p>
          </td>
        </tr>
      </table>
      <p>Une fois la connecton opérationnelle, cliquez sur le bouton &#34;Entrez dans l&#39;expérience de réalité
        augmentée&#34;. Il vous sera proposé d&#39;installer ARCore.</p>
      <p><img style="max-width: 233.50px" src="img/d9fa833e7c75fbf8.png"></p>
      <p>La première fois que vous lancez une application web AR, une boît de dialogue concernant les autorisations
        d&#39;accès à votre appareil photo apparaîtra.</p>
      <table>
        <tr>
          <td colspan="1" rowspan="1">
            <p><img style="max-width: 298.00px" src="img/32d7ef08a7216eb8.png"></p>
          </td>
          <td colspan="1" rowspan="1">
            <p><img style="max-width: 137.50px" src="img/92b0afd1dc7915e.png"></p>
          </td>
        </tr>
      </table>
      <p>Une fois que tout fonctionne bien, vous devriez voir une scène avec des cubes surplombant le flux vidéo de
        votre appareil photo. La compréhension de l‘environnement s&#39;améliore à mesure que vous vous déplacez et
        bougez votre téléphone, cela aide à stabiliser les choses.</p>
      <p><img style="max-width: 624.00px" src="img/bde12aa3ef3aeb7a.png"></p>
      <aside class="warning">
        <p><strong>Important</strong> : Pour des raisons de sécurités, l&#39;API WebXR n&#39;est utilisable que sur un
          domaine en HTTPS, avec une exception pour localhost. Si vous avez des difficultées à activer WebXR, vérifiez
          que vous êtes bien en HTTPS ou sur une adresse en localhost .</p>
      </aside>
      <aside class="special">
        <p>A partir de ce point, tous les tests-vérifications requièrent de visiter le lien sur votre téléphone.</p>
      </aside>


    </google-codelab-step>

    <google-codelab-step label="Statut de la réalité augmentée dans le web" duration="5">
      <h2><strong>Histoire</strong></h2>
      <p><a href="https://developer.mozilla.org/docs/Web/API/WebGL_API" target="_blank">WebGL</a> est librairie
        graphique puissante permettant le rendu de contenu 3D dans un navigateur web, mais l&#39;accès aux
        périphériques VR est nécessaire pour connaître et se synchroniser avec la fréquence d&#39;affichage des écrans.
        L&#39;API expérimentale <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebVR_API" target="_blank">WebVR
          1.1</a> a été implémentée dans les navigateurs au fur et à mesure que les développeurs construisaient des
        application VR pour le web. Cela a posé les bases permettant le rendu stéréoscopique de scène web avec les
        distorsions appropriées aux casques VR. Les périphériques compatibles <a href="https://vr.google.com/daydream/"
          target="_blank">Daydream</a> et <a href="http://www.samsung.com/global/galaxy/gear-vr/" target="_blank">GearVR</a>
        mais aussi les périphériques comme l&#39;<a href="https://www.oculus.com/rift/" target="_blank">Oculus Rift</a>
        ou <a href="https://www.vive.com/" target="_blank">HTC Vive</a> sont ceux supportés par la majorité des
        navigateurs.</p>
      <p>Les cas d&#39;usages évoluant, le support de l&#39;AR dans le web devint une nécessité. A cause d&#39;une
        grande similarité entre les technologies AR et VR, l&#39;API <a href="https://immersive-web.github.io/webxr/"
          target="_blank">WebXR</a> fut créée pour englober et supporter ces 2 domaines. Malgré son statut en cours de
        développement et donc sujette à des changements, le coeur de l&#39;API est stable, et est en cours de
        développement et d&#39;implémentation par la majorité des éditeurs de navigateurs web. L&#39;API supporte les
        expériences VR, et les propositions initiales AR sont en cours de prototypages et d&#39;exploration. </p>
      <h2><strong>Implémentation</strong></h2>
      <p>La première implémentation de l&#39;API WebXR est disponible dans Chrome depuis la version 67 derrière un
        drapeau (<code>#webxr</code>) mais aussi sur une &#34;<a href="https://github.com/GoogleChrome/OriginTrials"
          target="_blank"><em>origin trial</em></a>&#34;. Les fonctions AR initiales sont disponibles dans Chrome
        depuis la version 70 derrière un drapeau (<code>#webxr-hit-test</code>). A l&#39;heure de l&#39;écriture de ce
        codelab, la majorité des navigateurs web supportant l&#39;API WebVR ont assurés de leur support dans le futur
        de l&#39;API WebXR.</p>
      <h2><strong>Le futur</strong></h2>
      <p>La seule fonctionnalité de compréhension d&#39;une scène actuellement disponible dans un navigateur est le
        &#34;test de collision&#34;. Cela permet &#34;to cast a ray out from the device&#34;, c&#39;est-à-dire, à
        partir d&#39;un tapotement de doigt sur l&#39;écran, de retourner les coordonnées des collisions découvertes
        dans le monde réel, nous donnant la possibilité d&#39;utiliser ces informations pour afficher des couches
        virtuelles.</p>
      <p>Les explorations futures permettront une meilleure compréhension des scènes, comme l&#39;estimation de
        luminosité, des surfaces, nuages de points etc.</p>


    </google-codelab-step>

    <google-codelab-step label="Structure d&#39;une application AR" duration="15">
      <p>Placez-vous dans le dossier de travail <code>work</code>.</p>
      <p>Nous vous fournissons une base de travail avec une page HTML, du style CSS et un fichier JavaScript. Cette
        base vous permet également d&#39;activer les fonctionnalités AR d&#39;affichage et de rendu en synchro avec la
        position du téléphone.</p>
      <aside class="special">
        <p>Nous fournissons cette base de travail pour vous permettres de démarrer rapidement le codelab et de vous
          focaliser sur la partie AR. Si vous êtes déjà familier avec l&#39;API WebXR, passez à la section suivante.</p>
      </aside>
      <h2><strong>La page HTML</strong></h2>
      <p>Nous allons concevoir une expérience AR avec une page web traditionnelle en utilisant les technologies web.
        Dans cette expérience, nous allons utiliser une canvas en plein écran, notre page HTML n&#39;a donc pas besoin
        d&#39;une grande complexité de code. La partie CSS s&#39;assure que notre <code>&lt;canvas&gt;</code> injecté
        par notre librairie graphique est bien en plein écran.</p>
      <p>Les fonctionnalités AR requiert une interaction utilisateur minium de type gestuelle pour démarrer, nous
        utilisons donc certains éléments <a href="https://getmdl.io/" target="_blank">Material Design Lite</a> pour
        afficher un bouton <em>&#34;Démarrez l&#39;expérience&#34;</em> ou un message de <em>&#34;non support&#34;</em>
        du navigateur.</p>
      <p>La page <code>index.html</code> qui se trouve dans le dossier <code>work</code> doit ressembler à cela. (Ceci
        est un exemple de code, préférez le code venant du dépôt Github qui sera peut-être plus à jour, ne copiez pas
        ce code dans votre fichier):</p>
      <pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&#34;utf-8&#34;&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34;&gt;
  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1.0&#34;&gt;
  &lt;title&gt;Building an augmented reality application with the WebXR Device API&lt;/title&gt;
  &lt;link rel=&#34;stylesheet&#34; type=&#34;text/css&#34; href=&#34;../shared/app.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; type=&#34;text/css&#34; href=&#34;../third_party/mdl/material.min.css&#34;&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div id=&#34;enter-ar-info&#34; class=&#34;demo-card mdl-card mdl-shadow--4dp&#34;&gt;
      &lt;!-- Material Design elements for demo --&gt; 
      &lt;!-- ... --&gt;
  &lt;/div&gt;
  &lt;script src=&#34;../third_party/three.js/three.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;../shared/utils.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;app.js&#34;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
      <h2><strong>Découvrez la base de code JavaScript</strong></h2>
      <p>Notre application est un mélange d&#39;utilisation de la librairie 3D JavaScript <a href="http://threejs.org/"
          target="_blank">three.js</a>, quelques fonctions utilitaires et du code spécifique WebXR dans <code>app.js</code>.
        Découvrons en détail de squelette de code.</p>
      <aside class="special">
        <p>Ce squelette de code utilise la syntaxe async/await. Si vous n&#39;êtes pas familier avec cette syntaxe,
          lisez cet article : <a href="https://developers.google.com/web/fundamentals/primers/async-functions" target="_blank">Async
            functions - making promises friendly</a></p>
      </aside>
      <aside class="warning">
        <p>L&#39;API WebXR étant en constante évolution, le code source de ce codelab reflète la version de fin octobre
          2018, disponible ici : <a href="https://web.archive.org/web/20181031042404/https://immersive-web.github.io/webxr/"
            target="_blank">https://web.archive.org/web/20181031042404/https://immersive-web.github.io/webxr/</a></p>
      </aside>
      <p>Votre dossier work contient un fichier <code>app.js</code> dans lequel vous allez trouver la classe <code>App</code>
        :</p>
      <pre><code>class App {
  constructor() {
    ...
  }

  async init() {
    ...
  }

  async onEnterAR() {
    ...
  }

  onNoXRDevice() {
    ...
  }

  async onSessionStarted(session) {
   ...
  }

  onXRFrame(time, frame) {
    ...
  }
};

window.app = new App();</code></pre>
      <p>Nous instancions notre application et la stockons dans <code>window.app</code> par facilité pour le débogage
        avec <a href="https://developer.chrome.com/devtools" target="_blank">Chrome DevTools</a>.</p>
      <p>Notre constructeur appelle <code>this.init()</code> qui est une fonction async qui va démarrer notre session
        <a href="https://immersive-web.github.io/webxr/#xrsession-interface" target="_blank">XRSession</a> pour
        travailler en AR.</p>
      <p>Cette fonction vérifie l&#39;existence de <code>navigator.xr</code>, le point d&#39;entrée de l&#39;API WebXR,
        mais aussi de la présence de <code>XRSession.prototype.requestHitTest</code>, la fonctionnalité AR activée par
        le drapeau Chrome <code>webxr-hit-test</code>.</p>
      <ul>
        <li>si les 2 objets existent, nous pouvons demander la mise à disposition du device, via <code>navigator.xr.requestDevice()</code>
          qui retourne une promesse qui renvoie une <a href="https://immersive-web.github.io/webxr/#xrdevice-interface"
            target="_blank">XRDevice</a>, ou sinon échoue si non trouvé.</li>
        <li>si un des 2 objets n&#39;existent pas, nous appelons la méthode <code>this.onNoXRDevice()</code> qui
          affiche un message de &#34;non support AR&#34; à l&#39;utilisateur.</li>
      </ul>
      <p>Si tout va bien, nous connectons le click du bouton pour tenter de créer ensuite une session XR avec un
        écouteur de click.</p>
      <pre><code>class App {
  ...
  async init() {
    if (navigator.xr &amp;&amp; XRSession.prototype.requestHitTest) {
      try {
        this.device = await navigator.xr.requestDevice();
      } catch (e) {
        this.onNoXRDevice();
        return;
      }
    } else {
      this.onNoXRDevice();
      return;
    }

    document.querySelector(&#39;#enter-ar&#39;).addEventListener(&#39;click&#39;, this.onEnterAR);
  }
}</code></pre>
      <aside class="special">
        <p>Même si l&#39;API WebXR est supportée par le navigateur, cela ne veut pas dire que c&#39;est une XRDevice
          valide. Par exemple, un navigateur desktop peut implémenter l&#39;API mais ne pas avoir de matériel AR ou VR
          de connecté pour supporter l&#39;expérience. Lisez-en plus sur l&#39;énumération des périĥériques dans la <a
            href="https://immersive-web.github.io/webxr/#deviceenumeration" target="_blank">spécification WebXR</a>.</p>
      </aside>
      <p>Quand nous avons détecté une <code>XRDevice</code>, nous la stockons dans la propriété <code>this.device</code>.
        Dans le but d&#39;interagir avec ce périphérique, nous devons lui demander une XRSession. Une XRDevice peut
        avoir plusieurs XRSessions, dans chaque session est exposé la position du périphérique, l&#39;environnement de
        l&#39;utilisateur et gère le rendu du périphérique.</p>
      <p>Nous voulons que la sortie de cette session soit affichée dans notre page, nous devons donc créer un <code>XRPresentationContext</code>,
        similaire à un contexte WebGL <code>WebGLRenderingContext</code> si nous faisions nous même directement un
        rendu WebGL.</p>
      <pre><code>class App {
  ...
  async onEnterAR() {
    const outputCanvas = document.createElement(&#39;canvas&#39;);
    const ctx = outputCanvas.getContext(&#39;xrpresent&#39;);
    
    try {
      const session = await this.device.requestSession({
        outputContext: ctx,
        environmentIntegration: true,
      });
      document.body.appendChild(outputCanvas);
      this.onSessionStarted(session);
    } catch (e) {
      this.onNoXRDevice();
    }
  }
}</code></pre>
      <p>En appelant <code>getContext(‘xrprese</code>nt&#39;) sur notre canvas, cela va nous renvoyer un <code>XRPresentationContext</code>,
        qui est le contexte dans lequel le rendu de notre périphérique AR va être affiché. Ensuite nous demandons une
        session via <code>requestSession()</code> sur notre périphérique XR avec notre contexte de présentation, en
        ajoutant le drapeau <code>environmentIntegration</code> indiquant que nous voulons les fonctionnalités AR, et
        nous attendons la résolution de la promesse.</p>
      <ul>
        <li>si elle est résolue, nous ajoutons le canvas au DOM et appelons notre méthode <code>this.onSessionStarted()</code>
          avec notre nouvelle session <a href="https://immersive-web.github.io/webxr/#xrsession-interface" target="_blank">XRSession</a>.</li>
        <li>si elle échoue, nous affichons notre message d&#39;erreur avec la méthode <code>this.onNoXRDevice()</code></li>
      </ul>
      <aside class="warning">
        <p><strong>Important</strong> : avant d&#39;appeler <code>device.requestSession()</code> nous devons
          obligatoirement appeler <code>device.supportsSession()</code> avec nos options pour voir si notre
          configuration est supportée.</p>
      </aside>
      <p>Dès que nous avons notre XRSession, nous sommes prêt à mettre en place le rendu avec <a href="http://threejs.org/"
          target="_blank">three.js</a> et lancer notre boucle de rendu. Nous créons un WebGLRenderer three.js, qui va
        contenir notre second canvas, en s&#39;assurant que les options <code>alpha</code> et <code>preserveDrawingBuffer</code>
        sont activées. Nous utilisons le WebGLRederingContext venant de three.js et de manière asynchrone activons le
        <a href="https://immersive-web.github.io/webxr/#contextcompatibility" target="_blank">périphérique XR
          compatible</a>. Une fois le contexte considéré comme compatible avec le périphérique, nous pouvons créer une
        couche <a href="https://immersive-web.github.io/webxr/#xrwebgllayer-interface" target="_blank"><code>XRWebGLLayer</code></a>,
        et le mettre en place comme <code>baseLayer</code> de de notre XRSession. Cela dit à notre session que nous
        voulons utiliser ce contexte pour dessiner notre scène, qui va indirectement être affichée dans le canvas créé
        dans <code>this.init()</code>, mélanger avec le flux de notre caméra photo.</p>
      <p>Pour dessiner une scène three.js nous avons besoin de 3 composants : un WebGLRenderer pour gérer le rendu, une
        scène contenant des objets à afficher, et une caméra pour indiquer la perspective avec laquelle la scène doit
        être affichée. Nous allons utiliser une scène créée à partir de <code>DemoUtils.createCubeScene()</code> pour
        pré-remplir la scène avec des cubes flottant dans l&#39;espace. Si vous n&#39;avez pas travailler avec three.js
        ou WebGL avant, pas de soucis ! Si vous avez des problèmes de rendu, comparer votre code avec les examples.</p>
      <p>Avant que nous démarrions notre boucle de rendu, nous avons besoin d&#39;obtenir une trame de référence <a
          href="https://immersive-web.github.io/webxr/#xrframeofreference-interface" target="_blank">XRFrameOfReference</a>
        avec la valeur <code>‘eye-le</code>vel&#39;, indiquant que notre périphérique est en train de suivre sa
        position (en comparaison d&#39;expériences travaillant avec l&#39;orientation comme DayDream ou GearVR). Une
        fois que nous avons notre trame de référence, nous pouvons utiliser la méthode <code>requestAnimationFrame</code>
        de notre XRSession pour démarrer notre boucle de rendu, de manière similaire à <code>window.requestAnimationFrame</code>.</p>
      <pre><code>class App {
  ...
  async onSessionStarted(session) {
    this.session = session;

    document.body.classList.add(&#39;ar&#39;);
    
    this.renderer = new THREE.WebGLRenderer({
      alpha: true,
      preserveDrawingBuffer: true,
    });
    this.renderer.autoClear = false;

    this.gl = this.renderer.getContext();
    
    await this.gl.setCompatibleXRDevice(this.session.device);
  
    this.session.baseLayer = new XRWebGLLayer(this.session, this.gl);

    this.scene = DemoUtils.createCubeScene();

    this.camera = new THREE.PerspectiveCamera();
    this.camera.matrixAutoUpdate = false;

    this.frameOfRef = await this.session.requestFrameOfReference(&#39;eye-level&#39;);
    this.session.requestAnimationFrame(this.onXRFrame);
  }
}</code></pre>
      <aside class="special">
        <p>La méthode <code>requestAnimationFrame</code> de notre XRSession permet de s&#39;accrocher sur la fréquence
          d&#39;affichage du périphérique XRDevice. Les boucles de rendu des pages web standards sont conçues pour du
          60 FPS, alors que des périphériques externes vont faire des rendus en 120 FPS.</p>
      </aside>
      <p>Sur chaque image, <code>this.onXRFrame</code> est appelée avec un timestamp et une <a href="https://immersive-web.github.io/webxr/#xrpresentationframe"
          target="_blank">XRPresentationFrame</a>. De notre objet frame, nous obtenons un objet <a href="https://immersive-web.github.io/webxr/#xrdevicepose-interface"
          target="_blank">XRDevicePose</a>, qui décrit la position et l&#39;orientation dans l&#39;espace, et un
        tableau de vues XRViews, qui décrivent chaque point de vue à partir desquels nous devons dessiner la scène pour
        afficher correctement et dans le bon ordre les choses sur le périphérique.</p>
      <p>Premièrement, nous devons récupérer la position courante et s&#39;inscrire dans la file d&#39;attente
        d&#39;animation en appelant <code>session.requestAnimationFrame(this.onXRFrame)</code> avant que nous fassions
        un rendu. Les rendus stéréoscopique VR ont 2 vues (une pour chaque oeil), nous allons seulement afficher une
        vue comme nous affichons une expérience AR plein écran. Pour faire le rendu, nous allons boucler sur chaque vue
        et mettre en place la caméra en utilisant une matrice de projection en fonction des données de positionnement
        fournis. Cela synchronise la position virtuelle de la caméra et son orientation avec la position/orientation
        physique estimée de notre périphérique. Ensuite nous pouvons dire à notre renderer de faire le rendu de la
        scène avec la caméra appropriée.</p>
      <pre><code>class App {
  ...
  onXRFrame(time, frame) {
    const session = frame.session;
    const pose = frame.getDevicePose(this.frameOfRef);

    session.requestAnimationFrame(this.onXRFrame);

    this.gl.bindFramebuffer(this.gl.FRAMEBUFFER, this.session.baseLayer.framebuffer);

    if (pose) {
      for (let view of frame.views) {
        const viewport = session.baseLayer.getViewport(view);
        this.renderer.setSize(viewport.width, viewport.height);

        this.camera.projectionMatrix.fromArray(view.projectionMatrix);
        const viewMatrix = new THREE.Matrix4().fromArray(pose.getViewMatrix(view));
        this.camera.matrix.getInverse(viewMatrix);
        this.camera.updateMatrixWorld(true);

        this.renderer.clearDepth();

        this.renderer.render(this.scene, this.camera);
      }
    }
  }
}</code></pre>
      <p>Et c&#39;est tout... ;) </p>
      <p>Nous avons parcouru le code permettant :</p>
      <ul>
        <li>récupérer un périphérique XRDevice</li>
        <li>créer une session</li>
        <li>faire le rendu de la scène sur chaque image</li>
        <li>mettre à jour la position de notre caméra virtuelle avec la position physique estimée du périphérique</li>
      </ul>
      <h2><strong>Testez votre code</strong></h2>
      <p>Maintenant que nous avons parcouru ce squelette, découvrons le en action. Vous devriez voir le flux de votre
        caméra avec des cubes flottant dans l&#39;espace et dont la perspective changent quand vous vous déplacez. Le
        tracking s&#39;améliore au fur et à mesure que vous vous déplacez dans l&#39;espace.</p>
      <p><img style="max-width: 624.00px" src="img/bde12aa3ef3aeb7a.png"></p>
      <p><a href="https://vogloblinsky.github.io/webxr-codelab/work" target="_blank">
          <paper-button class="colored" raised>ESSAYEZ</paper-button>
        </a></p>
      <p>Si vous avez le moindre problème à cette étape, relisez les parties &#34;<strong>Introduction</strong>&#34; et
        &#34;<strong>Mise en place</strong>&#34;.</p>


    </google-codelab-step>

    <google-codelab-step label="Ajouter un cube sur une surface" duration="20">
      <p>Maintenant que nous avons notre flux vidéo de notre caméra, la position de notre périphérique et la position
        de la caméra et son orientation, et que nous affichons des cubes par-dessus, il est temps de commencer à
        interagir avec le monde réel en utilisant le &#34;test de collision&#34;. Nous voulons être capable de trouver
        une surface dans le monde réel, et de place un cube à cet endroit.</p>
      <h2><strong>Qu&#39;est ce qu&#39;un &#34;test de collision&#34; ?</strong></h2>
      <p>A &#34;test de collision&#34; est généralement un moyen de vérifier qu&#39;en traçant une ligne d&#39;un point
        de départ dans l&#39;espace avec une certaine direction, nous pouvons déterminer si cette ligne interagit avec
        des objets qui nous intéresse. Dans notre cas, nous allons tapper sur l&#39;écran de notre périphérique AR,
        donc il faut imaginer cette ligne partant de votre doigt, au travers de votre écran et parcourant le monde
        physique devant vous, monde que vous voyez au travers de votre écran à l&#39;aide de la caméra.</p>
      <p>L&#39;API WebXR va nous dire si cette ligne croise des objets du monde réel, ceci en utilisant des capacités
        AR. </p>
      <p>La magie opère ici au travers des SDK <a href="https://developers.google.com/ar/discover/" target="_blank">ARCore</a>
        (Google) et <a href="https://developer.apple.com/arkit/" target="_blank">ARKit</a> (Apple). L&#39;API WebXR
        contient en fait des branchements en natif vers ces SDK intégrés dans les sources des navigateurs, pour
        l&#39;instant ARCore pour Google Chrome. </p>
      <p><img style="max-width: 624.00px" src="img/d1f356be6b2f5fa8.png"></p>
      <h2><strong>Mise en place de la scène</strong></h2>
      <p>Nous allons ajouter une div dans la page <code>index.html</code> avec l&#39;ID <code>stabilization</code> pour
        afficher une animation à l&#39;utilisateur représentant un statut de stabilisation, lui demandant donc de
        bouger son téléphone. Cette animation sera affichée une fois en mode AR, et cachée dès que des surfaces auront
        été trouvées.</p>
      <pre><code>  &lt;div id=&#34;stabilization&#34;&gt;&lt;/div&gt;
  &lt;script src=&#34;../third_party/three.js/three.js&#34;&gt;&lt;/script&gt;
  ...
&lt;/body&gt;
&lt;/html&gt;</code></pre>
      <p>Maintenant dans notre fichier app.js, la première chose à faire est de supprimer nos cubes flottants. Dans la
        fonction onSessionStarted, remplacez <code>DemoUtils.createCubeScene()</code> par <code>new THREE.Scene()</code>.</p>
      <pre><code> // this.scene = DemoUtils.createCubeScene();
 this.scene = new THREE.Scene();</code></pre>
      <p>Juste après la création de la scène, nous avons besoin de créer un objet à placer durant notre future
        collision. Les objets à afficher en three.js sont représentés par des objets <code>THREE.Mesh</code>, qui
        contient une géométrie et un matériau. Voici le code pour créer cette géométrie et ce matériau, avec en même
        temps la
        transformation de notre cube pour que son origine soit sa face inférieure. Dès maintenant, nous avons juste
        créé le mesh, l&#39;avons modifié et stocké dans <code>this.model</code>.</p>
      <pre><code> const geometry = new THREE.BoxBufferGeometry(0.5, 0.5, 0.5);
 const material = new THREE.MeshNormalMaterial();
 geometry.applyMatrix(new THREE.Matrix4().makeTranslation(0, 0.25, 0));
 this.model = new THREE.Mesh(geometry, material);</code></pre>
      <p>Nous allons également fournir un objet three.js <code>Reticle</code> (le code source de création est dans
        <code>shared/utils.js</code>) qui va continuellement rechercher des hit-tests au centre de l&#39;écran, pour
        fournir un retour visuel à l&#39;utilisateur à propos de la compréhension du monde réel faite par le
        périphérique. Nous ajoutons le constructeur dans la fonction <code>onSessionStarted</code> après avoir récupéré
        la frame de référence. Dans notre fonction <code>onXRFrame</code>, nous appelons la méthode update du Reticle,
        et ajoutons la classe <code>stabilized</code> au body de la page dès que le reticle à trouver une surface. Cela
        cache également l&#39;animation d&#39;aide de l&#39;utilisateur.</p>
      <pre><code>class App {
  ...
  onSessionStarted(session) {
    ...
    this.reticle = new Reticle(this.session, this.camera);
    this.scene.add(this.reticle);
    
    this.frameOfRef = await this.session.requestFrameOfReference(&#39;eye-level&#39;);
    this.session.requestAnimationFrame(this.onXRFrame);
    ...
  }
  onXRFrame(time, frame) {
    let session = frame.session;
    let pose = frame.getDevicePose(this.frameOfRef);

    this.reticle.update(this.frameOfRef);
    if (this.reticle.visible &amp;&amp; !this.stabilized) {
      this.stabilized = true;
      document.body.classList.add(&#39;stabilized&#39;);
    }
    ...
  }
}</code></pre>
      <p>Nous voulons maintenant réaliser cete opération de hit-testing seulement sur une action utilisateur, une tap
        sur l&#39;écran par exemple, ajoutons donc un écouteur sur cet évènement :</p>
      <ul>
        <li>dans le constructeur, attachons la méthode de callback du click sur l&#39;instance</li>
        <li>écoutons l&#39;évènement click à la fin de la méthode <code>onSessionStarted</code>, pour s&#39;assurer que
          tout est supporté.</li>
        <li>finalement, ajoutons une méthode de classe vide de callback du click, <code>onClick</code></li>
      </ul>
      <pre><code>class App {
  constructor() {
    ...
    this.onClick = this.onClick.bind(this);
  }
  
  onSessionStarted(session) {
    ...
    this.reticle = new Reticle(this.session, this.camera);
    this.scene.add(this.reticle);

    window.addEventListener(&#39;click&#39;, this.onClick);
  }

  onClick(e) {
    console.log(&#39;click!&#39;);
  }
}</code></pre>
      <p>Vérifiez maintenant votre application, en s&#39;assurant dans Chrome Devtools que le log apparaît bien. Ce
        tutorial peut vous aider à mettre en place le débogage à distance d&#39;un périqphérique Android : <a href="https://developers.google.com/web/tools/chrome-devtools/remote-debugging/"
          target="_blank">https://developers.google.com/web/tools/chrome-devtools/remote-debugging/</a></p>
      <h2><strong>Déclencher le test de collision</strong></h2>
      <p>Maintenant que nous avons toute la mécanique pour le click et notre modèle 3D, il est temps de réaliser le
        test de collision. L&#39;API XRSession demande un point d&#39;origine, une direction et la XRFrameOfReference
        créée auparavant. Notre point d&#39;origine est la position du périphérique dans le système de coordonnées
        WebXR, et la direction est un vecteur de point partant de l&#39;arrière de notre périphérique. three.js a des
        fonctions utilitaires pour projeter un vecteur depuis un point, alors utilisons les. Cela peut paraître
        compliqué pour ceux qui ne sont pas familier avec les matrices et les vecteurs, mais la chose la plus
        importante ici est de comprendre que cela va générer une ligne partant de l&#39;arrière de votr téléphone. Nous
        pouvons utiliser un raycatser de three.js pour réaliser tout les calculs mathématiques, ce qui nous donne une
        origine et un vecteur de type <code>THREE.Vectors3</code>. Les positions x et y dans <code>setFromCamera</code>
        est la position dans l&#39;espace de l&#39;écran, et cette fonction prend des valeurs dans des coordonnées
        normalisées entre -1et 1. Come nous voulons tenter de placer un objet au milieu de l&#39;écran, nos valeurs x
        et y sont juste 0.</p>
      <p>La méthode <code>requestHitTest</code> retourne une promesse qui est résolue avec un tableau de <code>XRHitResults</code>,
        qui contient des matrices des positions ou des collisions ont eu lieu. Si notre ligne sortant de notre
        périphérique rencontre un mur ou le sol, la position de collision sera où cette ligne à croiser ce mur ou ce
        sol. Si nous trouvons une collision, nous prenons le premier (le plus proche) et le convertissons en <code>THREE.Matrix4</code>.
        Ensuite nous prenons le cube créé précédemment et le plaçons à la position de collision. Nous nous assurons
        aussi que le cube est ajouté à la scène pour qu&#39;il soit affiché. Comme maintenant nous devons utiliser
        await dans <code>onClick</code>, nous rendons <code>onClick</code> async.</p>
      <pre><code>class App {
  ...
  async onClick(e) {
    const x = 0;
    const y = 0;
   
    this.raycaster = this.raycaster || new THREE.Raycaster();
    this.raycaster.setFromCamera({ x, y }, this.camera);
    const ray = this.raycaster.ray;
    
    const origin = new Float32Array(ray.origin.toArray());
    const direction = new Float32Array(ray.direction.toArray());
    const hits = await this.session.requestHitTest(origin,
                                                   direction,
                                                   this.frameOfRef);

    if (hits.length) {
      const hit = hits[0];
      const hitMatrix = new THREE.Matrix4().fromArray(hit.hitMatrix);

      this.model.position.setFromMatrixPosition(hitMatrix);

      this.scene.add(this.model);
    }
  }
}</code></pre>
      <aside class="special">
        <p>Pour plus d&#39;informations sur la fonction de la XRSession <code>requestHitTest</code>, lisez la <a href="https://github.com/immersive-web/hit-test/"
            target="_blank">proposition en cours</a>. Comme tous les fonctionalités AR, cette API est sujette à des
          changements dans les implémentations futures.</p>
      </aside>
      <p>A ce point du codelab, nous avons gérer une interaction utilisateur, déclencher un test de collision. La
        sous-couche AR de Chrome, ARCore est capable de détecter des surfaces horizontales ou verticales.</p>
      <h2><strong>Testez votre code</strong></h2>
      <ul>
        <li>quand vous lancez votre application, vous devez pouvoir voir le reticle de recherche de surface. Sinon,
          déplacez votre téléphone.</li>
        <li>dès que vous voyez un reticle, en tapant vous devriez voir un cube placé à cet endroit</li>
        <li>en vous déplaçant, la couche ARCore détectera plus facilement les surfaces</li>
        <li>si vous rencontrez le moindre problème, vérifiez le code du dossier <strong>step-05/app.js</strong></li>
      </ul>
      <p><img style="max-width: 282.74px" src="img/c5225ea05ee284e5.png"></p>
      <p><a href="https://vogloblinsky.github.io/webxr-codelab/step-05" target="_blank">
          <paper-button class="colored" raised>ESSAYEZ</paper-button>
        </a></p>


    </google-codelab-step>

    <google-codelab-step label="Utiliser un modèle 3D" duration="20">
      <p>Nous avons maintenant mis en place la plupart de la mécanique de notre application. Il est très sympa
        d&#39;arriver à afficher des cubes mais nous pouvons sûrement faire mieux. Nous allons maintenant afficher un
        vrai modèle 3D provenant de <a href="https://poly.google.com/" target="_blank">Poly</a>, une collection
        d&#39;objets 3D proposés par des artistes, sous license <a href="https://support.google.com/poly/answer/7418679?hl=en"
          target="_blank">CC-BY</a>. Plusieurs modèles sont à disposition dans le dossier <strong><code>assets</code></strong>.</p>
      <p>Rajoutons d&#39;abord une référence concernant la license dans la page. Nous créons pour cela une div #info et
        ajoutons quelques informations sur la license :</p>
      <pre><code>&lt;div id=&#34;stabilization&#34;&gt;&lt;/div&gt;
  &lt;div id=&#34;info&#34;&gt;
    &lt;span&gt;
      Modèle 3D sous licence &lt;a href=&#34;https://creativecommons.org/licenses/by/2.0/&#34;&gt;CC-BY&lt;/a&gt;
    &lt;/span&gt;&lt;br /&gt;
  &lt;/div&gt;
  &lt;script src=&#34;../third_party/three.js/three.js&#34;&gt;&lt;/script&gt;
  ...
&lt;/body&gt;
&lt;/html&gt;</code></pre>
      <h2><strong>Chargement de fichiers OBJ et MTL</strong></h2>
      <p>Avant que nous commencions, nous allons utiliser des chargeurs de three.js : <a href="https://threejs.org/docs/#examples/loaders/OBJLoader"
          target="_blank">OBJLoader</a> pour le modèle et <a href="https://threejs.org/docs/#examples/loaders/MTLLoader"
          target="_blank">MTLLoader</a> pour les matériaux. Ces chargeurs ne sont pas inclus dans three.js, mais
        fournis séparément. Vous en avez une copie dans le dossier <code>third_party/three.js</code>, donc ajoutons les
        à notre page. Faites bien attention de les ajouter <strong>après</strong> three.js et <strong>avant</strong>
        app.js :</p>
      <pre><code>&lt;html&gt;
...
&lt;body&gt;
  ...
  &lt;script src=&#34;../third_party/three.js/three.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;../third_party/three.js/OBJLoader.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;../third_party/three.js/MTLLoader.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;../shared/utils.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;app.js&#34;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
      <p>Ajoutons des constantes référençant notre modèle au début de <code>app.js</code>.</p>
      <pre><code>const MODEL_OBJ_URL = &#39;../assets/muffin/muffin.obj&#39;;
const MODEL_MTL_URL = &#39;../assets/muffin/muffin.mtl&#39;;
const MODEL_SCALE = 0.1;</code></pre>
      <p>Maintenant, nous pouvons revenir à notre fonction <code>onSessionStarted</code>. Nous devons ajouter des
        lumières à notre scène three.js et charger notre modèle. Utilisons la fonction <code>DemoUtils.createLitScene()</code>
        qui va créer une scène <code>THREE.Scene</code> pour nous avec des lumières déjà incluses. Ensuite, utilisons
        un autre utilitaire, <code>DemoUtils.loadModel()</code>, qui utilise OBJLoader et MTLLoader que nous avons
        inclus précédemment. Cela retourne une promesse résolue avec le modèle en tant qu&#39;objet three.js, indiquant
        que le chargement est terminé, mais notez que nous n&#39;utilisons pas await car nous voulons continuer le
        rendu pendant le chargement du modèle. Dès que le modèle est chargé, nous le stockons dans this.model.</p>
      <pre><code>async onSessionStarted(session) {
  ...
  
  // this.scene = new THREE.Scene();
  this.scene = DemoUtils.createLitScene();
  
  // We no longer need our cube model.
  // Sorry, cube!
  /*
  const geometry = new THREE.BoxBufferGeometry(0.5, 0.5, 0.5);
  const material = new THREE.MeshBasicMaterial();
  geometry.applyMatrix(new THREE.Matrix4().makeTranslation(0, 0.25, 0));
  this.model = new THREE.Mesh(geometry, material);
  */

  DemoUtils.loadModel(MODEL_OBJ_URL, MODEL_MTL_URL).then(model =&gt; {
    this.model = model;
    this.model.scale.set(MODEL_SCALE, MODEL_SCALE, MODEL_SCALE);
  });

  ...
}</code></pre>
      <h2><strong>Placement de notre modèle</strong></h2>
      <p>Nous y sommes presque. La fonction de test de collision va placer this.model que nous avons remplacé par un
        modèle 3D maintenant, mais il reste quelques trucs à faire pour affiner tout ça. Dans le callback onClick,
        comme notre modèle est chargé de manière asynchrone, nous pourrions arriver dans un cas où nous
        l&#39;afficherions avant qu&#39;il soit chargé. Ajoutons un contrôle en début de fonction si this.model
        n&#39;existe pas.</p>
      <pre><code>async onClick(e) {
  if (!this.model) {
    return;
  }
  ...
}</code></pre>
      <h2><strong>Testez votre code</strong></h2>
      <p>Maintenant vous devriez voir en tapant sur votre écran votre modèle apparaître sur la surface détectée.</p>
      <p>Si vous rencontrez le moindre problème, vérifiez le code du dossier <strong>final/app.js</strong></p>
      <p><img style="max-width: 298.00px" src="img/5f9baa92feded1b2.png"></p>
      <p><a href="https://vogloblinsky.github.io/webxr-codelab/final" target="_blank">
          <paper-button class="colored" raised>ESSAYEZ</paper-button>
        </a></p>


    </google-codelab-step>

    <google-codelab-step label="BONUS : Utiliser un modèle animé" duration="0">
      <p>En changeant de loader, nous pouvons charger facilement un modèle 3D animé. Nous allons utilisé un loader de
        fichier GLTF, disponible dans le dossier <code>third_party/three.js</code>.</p>
      <pre><code>&lt;html&gt;
...
&lt;body&gt;
  ...
  &lt;script src=&#34;../third_party/three.js/three.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;../third_party/three.js/GLTFLoader.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;../shared/utils.js&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;app.js&#34;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
      <p>Dans votre fichier app.js, il faut modifier la référence vers le fichier gltf.</p>
      <pre><code>const MODEL_NAME = &#39;stormtrooper&#39;;

const MODEL_GLTF_URL = `../assets/${MODEL_NAME}/${MODEL_NAME}.gltf`;
const MODEL_SCALE = 0.25;</code></pre>
      <p>Pour animer notre modèle lors du rendu, nous allons instancier une référence à une horloge three.js et un
        mixer. Ce mixer servira à jouer l&#39;animation stockée dans le modèle, il sera instancié dans l&#39;appel de
        la méthode <code>loadGltfModel</code>.</p>
      <pre><code>const clock = new THREE.Clock();
let mixer = null;</code></pre>
      <p>Dans la méthode <code>onSessionStarted</code>, remplacez l&#39;appel de la méthode <code>loadModel</code> par
        <code>loadGltfModel</code></p>
      <pre><code>DemoUtils.loadGltfModel(MODEL_GLTF_URL).then(model =&gt; {
     this.model = model;
     // Every model is different -- you may have to adjust the scale
     // of a model depending on the use.
     this.model.scale.set(MODEL_SCALE, MODEL_SCALE, MODEL_SCALE);
});</code></pre>
      <p>Enfin, dans la méthode <code>onXRFrame</code>, nous allons lancer le rafraîchissement du mixer three.js avec
        l&#39;horloge.</p>
      <pre><code>this.gl.bindFramebuffer(this.gl.FRAMEBUFFER, this.session.baseLayer.framebuffer);

if (mixer) mixer.update(clock.getDelta());

if (pose) {
</code></pre>
      <h2><strong>Testez votre code</strong></h2>
      <p>Maintenant vous devriez voir en tapant sur votre écran un modèle de Stormtrooper apparaître sur la surface
        détectée.</p>
      <p>Si vous rencontrez le moindre problème, vérifiez le code du dossier <strong>bonus/app.js</strong></p>
      <p><img style="max-width: 320.00px" src="img/ezgif.com-optimize.gif"></p>
      <p><a href="https://vogloblinsky.github.io/webxr-codelab/bonus" target="_blank">
          <paper-button class="colored" raised>ESSAYEZ</paper-button>
        </a></p>


    </google-codelab-step>

    <google-codelab-step label="Crédits et ressources additionnels" duration="0">
      <p>Nous avons maintenant nos gâteaux chargés et affichés sur une surface dans le monde réel, avec des ombres. La
        méthode <code>requestHitTest</code> de l&#39;API WebXR est la première méthode implémentée permettant la
        compréhension du monde réel, mais dans le futur, il sera possible de mesurer la lumière ambiante, les surfaces
        et les informations de profondeur, mais aussi des nuages de points par exemple.</p>
      <p>Ces capacités vous nous permettre d&#39;intégrer encore plus facilement nos objets dans le monde réel. Par
        exemple, l&#39;estimation de lumière va s&#39;accorder avec les ombres du modèles dans le monde réel, et les
        données de profondeur nous donnerons la possibilité de cacher des objets numériques quand certains objets réels
        seront en face. Ce qui reste en cours de spécification dans l&#39;API WebXR est la notion d&#39;<a href="https://github.com/immersive-web/anchors"
          target="_blank">ancres</a>, avec la possibilité de suivre des objets du monde réel et plus précisément la
        position d&#39;objets dans une scène.</p>
      <h2><strong>Crédits additionnels : choisissez votre propre modèle</strong></h2>
      <p>Vous pouvez trouver d&#39;autres modèles sur les sites : <a href="https://poly.google.com/" target="_blank">Poly</a>
        et <a href="https://sketchfab.com/" target="_blank">SketchFab</a>.</p>
      <h2>Ressources</h2>
      <ul>
        <li><a href="https://immersive-web.github.io/webxr/" target="_blank">WebXR Device API Specification</a></li>
        <li><a href="https://github.com/immersive-web/webxr-samples" target="_blank">WebXR Samples</a></li>
        <li><a href="https://poly.google.com/" target="_blank">Poly</a></li>
        <li><a href="https://sketchfab.com/" target="_blank">SketchFab</a></li>
        <li><a href="https://github.com/immersive-web/hit-test" target="_blank">Hit Test proposal for WebXR</a></li>
        <li><a href="https://github.com/immersive-web/anchors" target="_blank">Anchor proposal for WebXR</a></li>
        <li><a href="http://threejs.org/" target="_blank">three.js</a></li>
      </ul>


    </google-codelab-step>

  </google-codelab>

  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-49880327-14', 'auto');

    (function() {
      var gaCodelab = '';
      if (gaCodelab) {
        ga('create', gaCodelab, 'auto', {
          name: 'codelab'
        });
      }

      var gaView;
      var parts = location.search.substring(1).split('&');
      for (var i = 0; i < parts.length; i++) {
        var param = parts[i].split('=');
        if (param[0] === 'viewga') {
          gaView = param[1];
          break;
        }
      }
      if (gaView && gaView !== gaCodelab) {
        ga('create', gaView, 'auto', {
          name: 'view'
        });
      }
    })();
  </script>

</body>

</html>